---
layout: page
title: CMPUT 503 Lab 3 report
permalink: 412-projects/A3/
date: 2025-02-12 10:33:00 -0000
---


# Part One - Computer Vision
1. Camera distortion
Subscribed to the `vehicle_name/camera_node/camera_info` topic get the following callback output.  
```
header: 
  seq: 24
  stamp: 
    secs: 1740071338
    nsecs: 196597099
  frame_id: "/csc22946/camera_optical_frame"
height: 480
width: 640
distortion_model: "plumb_bob"
D: [-0.25706255601943445, 0.045805679651939275, -0.0003584336283982042, -0.0005756902051068707, 0.0]
K: [319.2461317458548, 0.0, 307.91668484581703, 0.0, 317.75077109798957, 255.6638447529814, 0.0, 0.0, 1.0]
R: [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
P: [217.5807647705078, 0.0, 310.12126231584625, 0.0, 0.0, 256.01397705078125, 263.90036341759696, 0.0, 0.0, 0.0, 1.0, 0.0]
binning_x: 0
binning_y: 0
roi: 
  x_offset: 0
  y_offset: 0
  height: 0
  width: 0
  do_rectify: False
```

<img src="{{site.baseurl}}/assets/images/412-A3/cameraMatrix.png" alt="cameramatrix" width="100%" height="auto">

Where DKRP is the camera matrix.  
Duckiebot have pinhole camera model. 
`x = PX` where x is the coordinate in image plane. and X is the world coordinate. 
P is the transformation applied.  
`K` is the calibration (intrinsic) matrix and `D` is the distortion matrices.

### Distorted
<figure>
    <img src="{{site.baseurl}}/assets/images/412-A3/csc22946_distorted.png" alt="maes" width="100%" height="auto">
    <figcaption>Figure 1: distorted image. Simply a raw image gotten from the bot.</figcaption>
</figure>

### Undistorted
<figure>
    <img src="{{site.baseurl}}/assets/images/412-A3/csc22946_undistorted.png" alt="maes" width="100%" height="auto">
    <figcaption>Figure 2: undistorted image.</figcaption>
</figure>
To undistort the image, we grab the K and D matrices from the `camera_info` topic.
K is the intrinsic parameter matrices and D is the distortion coefficient matrices.
We use CV2's `undistort` method to do the job. 
Code [here](https://github.com/NicolasOng/CMPUT503/blob/97b6d48603757c7b47bc58ea302cae7dc585c03c/Exercise%203/exercise-3/packages/computer_vision/src/camera-reader.py#L48).

## Image recognition ([code line](https://github.com/NicolasOng/CMPUT503/blob/736a3f5be73d7448e287391532c6c7196b5b8faf/Exercise%203/exercise-3/packages/computer_vision/src/lane_detection.py#L109C1-L109C53))
1. We first find the lower and higher hsv value for color of interest. We did that manually using a color picker.
2. `hsvFrame = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2HSV)` convert cv2_img to HSV tensor.
3. we compute mask by activating all the pixels that are in range `color_mask = cv2.inRange(hsvFrame, self.red_lower, self.red_upper)`.  
Color mask when displayed
<img src="{{site.baseurl}}/assets/images/412-A3/color_mask_1.png" alt="colormask1" width="100%" height="auto">
4. Then we dilate the mask to enlarge color object. `color_mask = cv2.dilate(color_mask, kernel)`.  
Color mask when displayed
<img src="{{site.baseurl}}/assets/images/412-A3/color_mask_dilate.png" alt="colormask1" width="100%" height="auto">

By Creating `color_mask` for each of the  RED, BLUE, GREEN, WHITE, YELLOW, we can draw bouding box aroud them for visual purpose. 
<img src="{{site.baseurl}}/assets/images/412-A3/color_detection.png" alt="colormask1" width="100%" height="auto">

## Stopping before colored tapes 
To integrate computer vision, LED control, and wheel movement nodes, we used ROS services. 
The odometry and wheel movement nodes from lab 2 is in the [Move](https://github.com/NicolasOng/CMPUT503/blob/main/Exercise%203/exercise-3/packages/computer_vision/src/move.py) node. 
This node acts as a server that provides the services `{drive_straight/rotate/drive_arc/pause/drive_turn}_request`. 
The node [color_based_movement](https://github.com/NicolasOng/CMPUT503/blob/main/Exercise%203/exercise-3/packages/computer_vision/src/color_based_movement.py) is
the only client to use these services for the stopping tasks below.
The LED control does not have its own node.
It simply exist inside `color_based_movement` node.
Lastly, we have the node `camera_detection` node that uploads the coordinates of the nearest colored objects. 

<video controls style="width: 100%; height: 80%;">
    <source src="{{ site.baseurl }}/assets/videos/X.mp4" type="video/mp4">
    video stop before blue line.
</video>
<video controls style="width: 100%; height: 80%;">
    <source src="{{ site.baseurl }}/assets/videos/X.mp4" type="video/mp4">
    video stop before yellow line.
</video>
<video controls style="width: 100%; height: 80%;">
    <source src="{{ site.baseurl }}/assets/videos/X.mp4" type="video/mp4">
    video stop before red line.
</video>
We have the node [color_based_movement.py](https://github.com/NicolasOng/CMPUT503/blob/main/Exercise%203/exercise-3/packages/computer_vision/src/color_based_movement.py) to do the stopping tasks. 
In this node, we subscribe to `{vehicle_name}/color_coords` topic to get the the coordinate of nearest color object at any time. 
The publisher of this node is [camera_detection](https://github.com/NicolasOng/CMPUT503/blob/main/Exercise%203/exercise-3/packages/computer_vision/src/camera_detection.py) node. 
In the [movement()](https://github.com/NicolasOng/CMPUT503/blob/c05e30c53e4a1776b7c56bad86fe71153e76e8a1/Exercise%203/exercise-3/packages/computer_vision/src/color_based_movement.py#L170C9-L170C17) function, we have
 a loop to stop when the distance to these color tapes are close enough.

After the bot stops in front of the colored line, it executes next actions accordingly.
For example, if it stops in front of a red line, it will publish to `LEDcontrol` node
accordingly and calls the `drive_Straight` service with the json parameter  

`{
            "meters": meters,
            "speed": speed,
            "leds": leds}
`.  

We can improve the modularity of this integration by having a separate node for LED controls.
However, it will incur more latencies with additional service calls to the `LED` node.
To optimize for delays with service calls to the `move` node, we simply stopped using the move node.
Instead, all the movements are done using `Twist2dStamped` message. `&#128512;

To answer part one Q6 b) iv),  the higher camera frequency does not affect the integration.
The rate of camera processing is determined by the main control loop in `camera_detection` node.

## Lane Following Prerequisite: Error Computation
<figure>
    <img src="{{site.baseurl}}/assets/images/412-A3/maes_straight.png" alt="maes" width="100%" height="auto">
    <figcaption>Figure 1: Green line is precomputed line meant to align with white/yellow lines. Yellow/White lines are line of best fit computed from color detection. 
    Red shading is the errors that we compute.</figcaption>
</figure>
<figure>
    <img src="{{site.baseurl}}/assets/images/412-A3/maes_curve_degree1.png" alt="maes" width="100%" height="auto">
    <figcaption>Figure 1: Visualization of errors (red shading) when we encounter curved terrains using degree 1 regression on yellow and white lines.
    Red shading is the errors that we compute.</figcaption>
</figure>

<figure>
    <img src="{{site.baseurl}}/assets/images/412-A3/maes_curve_degree2.png" alt="maes" width="100%" height="auto">
    <figcaption>Figure 1: Visualization of errors (red shading) when we encounter curved terrains using degree 2 regression on yellow and white lines.
    Red shading is the errors that we compute.</figcaption>
</figure>

First we pre-define tangent lines on both side (the line in green). 
This tangent line represents the camera orientation with respect to white/yellow lines in order to follow lane. 

We detect all the yellow and white pixels in an image and compute the degree=1 
line of best fit for yellow and white detected lines. This returns the tangent line coefficient.

We compute 2 errors. An error between the detected yellow/white lines and the tangent lines on each side. 
We publish these 2 errors as `{yellow: yellow_error, white: white_error}` into `{vehicle_name}/maes` topic for the `controller` node to use.

## Controller
[!["P controller"](https://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](https://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE)

[!["PD controller"](https://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](https://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE)


[!["pid controller"](https://img.youtube.com/vi/youtube_video_id_here/0.jpg)](https://www.youtube.com/watch?v=youtube_video_id_here)

We have a [controller](https://github.com/NicolasOng/CMPUT503/blob/main/Exercise%203/exercise-3/packages/computer_vision/src/controller.py) node.
In this node, we subscribe to `{vehicle_name}/maes`, a the error values for the controller to minimize. 
Aforementioned, the publisher of this topic is the `camera_detection` node.

In the controller node, we have a [loop](https://github.com/NicolasOng/CMPUT503/blob/7831cedeca00a3a6a229d3dc592a17786b9af672/Exercise%203/exercise-3/packages/computer_vision/src/controller.py#L87) that outputs a steering angle value that we
clamp between $$[-2\pi, 2\pi]$$. This performs straight line task pretty easily.

## Pros, cons, differences between controllers
### P controller

**Pros**:  
-Only variable is the proportional gain $$K_p$$. It adds immediate feedback to the
errors. 
Simple to understand the behavior
**Cons**:  
-Sensitive to small gain changes. The bot overshoots during correction, causing high angles.
**Tune**:  
-Through trial and error, we tried values between -0.3 to 0. 
-Since the contribution of the P term toward steering angle is $$K_p \cdot error$$, we want the product to be negative if the error is positive (bot need to steer left, a negative angle) and vice versa.

### PD controller
**Pros**:  
-Reduces oscillations and improves stability by taking into account of the rate of change term (derivative). 
**Cons**:  
-Caused overshooting output when the error term increased rapidly when bots were turning. 
**Tuning**:  
-The contribution of the D term toward steering angle is $$K_d \cdot derivative$$.
So we want the sign of this product to be same as of the P term. 
Therefore, we se the value of $$K_p$$ term to be negative to oppose the rate of error change, dampening the oscillation.  

### PID controller
**Pros**:  
-In theory, eliminates steady-state error.  
**Cons**:  
-Hard to tune since we have to tune the previous 2 terms.  
**Tuning**:  
-We haven't tune the I term yet.   

## Lane Following
[![lane follow](https://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](https://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE)

### Challenges
The approach to compute errors described in lane following did not work well in the circle track. The reason is because the latency due from error computation is high despite building and running the container on the bot. When the bot approach the curve, the error changed rapidly. 
This requires the error value to be up-to-date. 
Otherwise, the bot will turn too much or turn too little on the curve, and this is exactly what happened. 
Another reason why it overshoots is that the `Twisted2DStamp` message changes the wheel velocity to accomodate big change in angles, exercerbating overshoot during turns with latencies.

### Solution
We implemented a simpler version of error computation for the controller in [perform_simple_camera_detection()](https://github.com/NicolasOng/CMPUT503/blob/97b6d48603757c7b47bc58ea302cae7dc585c03c/Exercise%203/exercise-3/packages/computer_vision/src/camera_detection.py#L630).
We only follow the outer white line. 

In that function, we crop the camera image to bottom left or bottom right depending on where the white line is.

<figure>
    <img src="{{site.baseurl}}/assets/images/412-A3/xxx.png" alt="maes" width="100%" height="auto">
    <figcaption>Figure 1: Cropping of the image to focus on the white line</figcaption>
</figure>

Then we find the center point of the white-lane bounding-box.
The error is the difference between the center of the white-lane bounding box 
and the predefined point in within camera image that we want the bot to follow. 
Look [here](https://github.com/NicolasOng/CMPUT503/blob/97b6d48603757c7b47bc58ea302cae7dc585c03c/Exercise%203/exercise-3/packages/computer_vision/src/camera_detection.py#L653) for the detail. 

Then Voila! We can follow the circle using same $$K_p, K_d, K_i$$ as the straight line task. 

[![lane follow](https://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](https://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE)


##