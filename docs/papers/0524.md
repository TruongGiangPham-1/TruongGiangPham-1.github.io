---
layout: page
title: Hybrid computing using a neural network with dynamic external memory
permalink: paperreview/01
date: 2025-05-24 10:33:00 -0000
---

Paper link : https://gwern.net/doc/reinforcement-learning/model-free/2016-graves.pdf


## Introduction
Neural network does not separate computation and memory like modern computer. 
This paper introduces external memory matrix $$M \in R^{N x W}$$ that the Atten based neural network can 
interacts with using diffrientiable attentions read and writes.

This is a 2016 paper, one year before the invention of current transformer. 

The weakness of the architecture is that it can accidently delete things in the memory space that it might need later.
THe memory can be thought of as a KV cache, 

THink of it as CPU funciton unit like ALU is replaced by a neural network. So arthmetic / read/write operations
are learned. Operations are diffrientiable and continouous.

## How it works

### Read head
Has read key which asks which row should of M should we read from. 
Returns a read vector $$\sum_{i}{N} M[i, ]\cdot w^r[i] $$ with read weighting $$w^r$$.

How is the read weighting learned.

### Write head
Has write key which asks which row should of M should we write to.
Has eras key to which is not good
Writes M[i, j] = M[i, j]

### More attention heads
**lookup**:

**Shift**:
L matrix is like positional encoding, allows write to move throught he meomry space.


## Limitation
How explicit is memory addressing is. Memory addressing is done using softmax. Tiny error ican write something to wrong location.
Ultimately, it cause cascading error. 
Handling physical state(Memory) is super challenging. 
Having readable and writeble ram means the functions is not reproduceble. 
Having mutable state is not good for computation.
Language like python rust get rid of memory management. To get rid of class of bugs that comes with it.
LLM needing to learn this mutable memory space is programatic. Not scalable. 


LSTM is different as it always replace it with new state. removes cascading error. 

## Turing machine discussion
Being turing complete is not a high burnden to reach. 
What can you do to not be turing complete, allows you to check whether search has complete -> proof for halt


if a system that can generate turing complete code, that doesnt mean the system is turing complete.


move instruction in x86 is turing complete. 
set of production rule in LLM is not sophisticated enough to be turing complete.
Halting problem is turing complete. 

ZickZack argue that LLM is just a regular language

Calculus of construction
Lambda cube 